{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading file FDD天线安装规划(3000323).csv: Usecols do not match columns, columns expected but not found: ['远端射频单元编号', '网元标识', '本地小区标识1']\n",
      "Error reading file TDD天线安装规划(3000323).csv: Usecols do not match columns, columns expected but not found: ['远端射频单元编号', '网元标识', '本地小区标识1']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 0 elements, new values have 3 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 101\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Read and process data\u001b[39;00m\n\u001b[0;32m    100\u001b[0m df_AP \u001b[38;5;241m=\u001b[39m read_and_process_files(Datadir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m天线安装规划\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m网元标识\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m远端射频单元编号\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m本地小区标识1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 101\u001b[0m \u001b[43mdf_AP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m网元标识\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m射频单元编号\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m小区本地ID\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Rename columns for df_AP\u001b[39;00m\n\u001b[0;32m    103\u001b[0m gdffile \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m湖北省村级边界.geojson\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    104\u001b[0m gdf \u001b[38;5;241m=\u001b[39m read_hubei_map_files(Datadir, gdffile)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\PYMo\\SuperMO\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:6313\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   6311\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   6312\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n\u001b[1;32m-> 6313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m   6315\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mproperties.pyx:69\u001b[0m, in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\PYMo\\SuperMO\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:814\u001b[0m, in \u001b[0;36mNDFrame._set_axis\u001b[1;34m(self, axis, labels)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;124;03mThis is called from the cython code when we set the `index` attribute\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;124;03mdirectly, e.g. `series.index = [1, 2, 3]`.\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    813\u001b[0m labels \u001b[38;5;241m=\u001b[39m ensure_index(labels)\n\u001b[1;32m--> 814\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\PYMo\\SuperMO\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:238\u001b[0m, in \u001b[0;36mBaseBlockManager.set_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: AxisInt, new_labels: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;66;03m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_set_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis] \u001b[38;5;241m=\u001b[39m new_labels\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\PYMo\\SuperMO\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\base.py:98\u001b[0m, in \u001b[0;36mDataManager._validate_set_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m new_len \u001b[38;5;241m!=\u001b[39m old_len:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength mismatch: Expected axis has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements, new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length mismatch: Expected axis has 0 elements, new values have 3 elements"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Unified data directories\n",
    "Datadir = r'C:\\Users\\Administrator\\Documents\\MnewData'\n",
    "output_path = r'C:\\Users\\Administrator\\PYMo\\Data'\n",
    "\n",
    "def list_csv_files(directory, keyword):\n",
    "    \"\"\"List CSV files in the specified directory that contain the keyword.\"\"\"\n",
    "    return [file for file in os.listdir(directory) if keyword in file and file.endswith('.csv')]\n",
    "\n",
    "def read_and_process_files(directory, keyword, columns=None, encoding='gbk'):\n",
    "    \"\"\"Read and process CSV files, returning a concatenated DataFrame.\"\"\"\n",
    "    files = list_csv_files(directory, keyword)\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding=encoding, usecols=columns,na_values=[\"n/a\", \"na\", \"-\"]) if columns else pd.read_csv(file_path, encoding=encoding,skiprows=2, header=0, na_values=[\"n/a\", \"na\", \"-\"])\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file}: {e}\")\n",
    "    return pd.concat(dfs, ignore_index=True).drop_duplicates() if dfs else pd.DataFrame()\n",
    "\n",
    "def read_hubei_map_files(directory, filename):\n",
    "    \"\"\"Read the Hubei map file.\"\"\"\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    return gpd.read_file(file_path)\n",
    "\n",
    "def split_field(x, is_rru):\n",
    "    \"\"\"Split the '对象' field into relevant components.\"\"\"\n",
    "    return x.split(',')  # Adjust the split logic as per actual data format\n",
    "\n",
    "def process_rru_pl(RRU_PL):\n",
    "    \"\"\"Process RRU_PL DataFrame.\"\"\"\n",
    "    RRU_PL[['BBUID', 'RRUID']] = RRU_PL['对象'].apply(lambda x: pd.Series(split_field(x, is_rru=True)))\n",
    "    RRU_PL = RRU_PL[['BBUID', 'RRUID', '开始时间', 'AAU功耗[千瓦时]']]\n",
    "    RRU_PL['AAU功耗[千瓦时]'] = pd.to_numeric(RRU_PL['AAU功耗[千瓦时]'], errors='coerce').round(4).fillna(0)\n",
    "    return RRU_PL\n",
    "\n",
    "def process_bbu_pl(BBU_PL):\n",
    "    \"\"\"Process BBU_PL DataFrame.\"\"\"\n",
    "    BBU_PL[['BBU名称', 'BBUID']] = BBU_PL['对象'].apply(lambda x: pd.Series(split_field(x, is_rru=False)))\n",
    "    BBU_PL['站型'] = BBU_PL['BBU名称'].apply(lambda x: '宏站' if 'D5H' in x else ('微站' if 'D5M' in x else ('室分' if 'D5S' in x else '未知')))\n",
    "    BBU_PL = BBU_PL[['BBU名称', 'BBUID', '站型', '开始时间', 'BBU功耗[千瓦时]', 'gNB基站CPU平均负荷(R1056_001)[%]', 'gNB基站CPU峰值负荷(R1056_002)[%]', 'BBU功耗(R1054_001)[W]']]\n",
    "    \n",
    "    for col in ['BBU功耗[千瓦时]', 'BBU功耗(R1054_001)[W]']:\n",
    "        BBU_PL[col] = pd.to_numeric(BBU_PL[col], errors='coerce').round(4).fillna(0)\n",
    "    \n",
    "    for col in ['gNB基站CPU平均负荷(R1056_001)[%]', 'gNB基站CPU峰值负荷(R1056_002)[%]']:\n",
    "        BBU_PL[col] = pd.to_numeric(BBU_PL[col], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    return BBU_PL\n",
    "\n",
    "def process_rf_ap_cp(df_ap, df_rf, df_cp, gdf):\n",
    "    \"\"\"Merge RF, AP, and CP data and perform spatial join.\"\"\"\n",
    "    df_rfap = pd.merge(df_ap, df_rf, on=['网元标识', '射频单元编号'], how='inner')\n",
    "    df_rfap = df_rfap[['网元标识', '小区本地ID', '射频单元编号', 'Longitude', 'Latitude']]  # Select and rename columns\n",
    "\n",
    "    df_rac = pd.merge(df_rfap, df_cp, on=['网元标识', '小区本地ID'], how='inner')\n",
    "    df_rac = df_rac[['网元标识', '小区本地ID', 'BBU机房', '基站名称', '小区名称', '工作频段', 'Longitude', 'Latitude']]  # Select and rename columns\n",
    "\n",
    "    geometry = [Point(xy) for xy in zip(df_rac['Longitude'], df_rac['Latitude'])]\n",
    "    df_rac = gpd.GeoDataFrame(df_rac, geometry=geometry)\n",
    "\n",
    "    return gpd.sjoin(df_rac, gdf, how='inner', predicate='within')\n",
    "\n",
    "def process_bs_pl(BBU_PL, RRU_PL):\n",
    "    \"\"\"Process BS_PL data.\"\"\"\n",
    "    antenna_count = RRU_PL.groupby(['BBUID', '开始时间'])['RRUID'].nunique().reset_index(name='天线数量')\n",
    "    total_rru_power = RRU_PL.groupby(['BBUID', '开始时间'])['AAU功耗[千瓦时]'].sum().reset_index(name='RRU总功耗')\n",
    "\n",
    "    BBU_PL = pd.merge(BBU_PL, antenna_count, on=['BBUID', '开始时间'], how='left')\n",
    "    BBU_PL = pd.merge(BBU_PL, total_rru_power, on=['BBUID', '开始时间'], how='left')\n",
    "    BBU_PL['频段'] = BBU_PL['BBU名称'].apply(lambda x: '700M' if '700M' in x else '2.6G')\n",
    "\n",
    "    return BBU_PL[['BBU名称', 'BBUID', '天线数量', 'RRU总功耗', '频段']]\n",
    "\n",
    "def save_dataframe_to_csv(df, filename):\n",
    "    \"\"\"Save DataFrame to CSV file.\"\"\"\n",
    "    df.to_csv(os.path.join(output_path, filename), index=False, encoding='utf-8-sig')\n",
    "    print(f\"DataFrame exported to {filename}\")\n",
    "\n",
    "# Read and process data\n",
    "df_AP = read_and_process_files(Datadir, '天线安装规划', ['网元标识', '远端射频单元编号', '本地小区标识1'])\n",
    "df_AP.columns = ['网元标识', '射频单元编号', '小区本地ID']  # Rename columns for df_AP\n",
    "\n",
    "gdffile = '湖北省村级边界.geojson'\n",
    "gdf = read_hubei_map_files(Datadir, gdffile)\n",
    "\n",
    "df_RF = read_and_process_files(Datadir, '射频单元规划', ['网元标识', '射频单元编号', '射频单元RRU安装经度', '射频单元RRU安装纬度'])\n",
    "df_RF.columns = ['网元标识', '射频单元编号', 'Longitude', 'Latitude']  # Rename columns for df_RF\n",
    "\n",
    "df_CP = read_and_process_files(Datadir, 'NR小区', ['网元标识', '网元名称', '小区本地ID', '小区友好名', 'Nr小区工作频段'])\n",
    "df_CP.rename(columns={'小区友好名': '小区名称', '网元名称': 'BBU机房'}, inplace=True)\n",
    "df_CP['工作频段'] = df_CP['Nr小区工作频段'].str.split('(').str[0]\n",
    "df_CP['基站名称'] = df_CP['小区名称'].str.replace(r'(-26.*|-07.*)', '', regex=True)\n",
    "df_CP = df_CP[['网元标识', 'BBU机房', '基站名称', '小区本地ID', '小区名称', '工作频段']]\n",
    "\n",
    "gdf_RAC = process_rf_ap_cp(df_AP, df_RF, df_CP, gdf)\n",
    "\n",
    "# Read BBU and RRU power consumption\n",
    "BBU_PL = read_and_process_files(Datadir, 'DT_BBU功耗_')\n",
    "RRU_PL = read_and_process_files(Datadir, 'DT_RRU功耗_')\n",
    "\n",
    "# Process RRU_PL and BBU_PL\n",
    "RRU_PL = process_rru_pl(RRU_PL)\n",
    "BBU_PL = process_bbu_pl(BBU_PL)\n",
    "\n",
    "# Process BS_PL\n",
    "BS_PL = process_bs_pl(BBU_PL, RRU_PL)\n",
    "\n",
    "# Process KPI file\n",
    "df_KPI = read_and_process_files(Datadir, 'DT_PowerBI指标通报计数器_')\n",
    "\n",
    "# Save results\n",
    "save_dataframe_to_csv(gdf_RAC, 'gdf_RAC.csv')\n",
    "save_dataframe_to_csv(df_KPI, 'df_KPI.csv')\n",
    "save_dataframe_to_csv(BS_PL, 'BS_PL.csv')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
